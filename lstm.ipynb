{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import re\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets():\n",
    "    src = [\n",
    "        \"astroturf\",\n",
    "        \"kevin_feedback\",\n",
    "        \"botwiki\",\n",
    "        # \"zoher-organization\",\n",
    "        # \"cresci-17\",\n",
    "        # \"rtbust\",\n",
    "        # \"stock\",\n",
    "        # \"gilani-17\",\n",
    "        # \"midterm-2018\",\n",
    "        # \"josh_political\",\n",
    "        # \"pronbots\",\n",
    "        # \"varol-icwsm\",\n",
    "        # \"gregory_purchased\",\n",
    "        # \"verified\"\n",
    "    ] \n",
    "    \n",
    "    all_data = []\n",
    "    for dataset_name in src:\n",
    "        filepath = f\"datasets/{dataset_name}.json\"\n",
    "        with open(filepath, \"r\") as file:\n",
    "            data = json.load(file) \n",
    "        all_data.extend(data)\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "data = read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original class distribution:\")\n",
    "print(data['user_class'].value_counts())\n",
    "\n",
    "# Separate data by user_class\n",
    "bots = data[data['user_class'] == 'bot']\n",
    "humans = data[data['user_class'] == 'human']\n",
    "\n",
    "# Select the minimum class size\n",
    "min_class_size = min(len(bots), len(humans))\n",
    "\n",
    "# Downsample each class to the minimum class size\n",
    "bots_balanced = bots.sample(n=min_class_size, random_state=1)\n",
    "humans_balanced = humans.sample(n=min_class_size, random_state=1)\n",
    "\n",
    "# Combine the balanced classes\n",
    "balanced_data = pd.concat([bots_balanced, humans_balanced])\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_data = shuffle(balanced_data, random_state=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced class distribution:\")\n",
    "print(balanced_data['user_class'].value_counts())\n",
    "\n",
    "data = balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data shape\", data.shape)\n",
    "print(\"columns\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_action_to_content(action_string, content_string):\n",
    "    \"\"\"\n",
    "    Maps each valid character in the action string to the corresponding group in the content string,\n",
    "    skipping non-character symbols and formatting the output as specified.\n",
    "\n",
    "    Args:\n",
    "    action_string (str): A string where each character represents an action.\n",
    "    content_string (str): A string containing groups inside parentheses.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string where each action is paired with a content group.\n",
    "    \"\"\"\n",
    "    # Parse content_string to extract groups inside parentheses\n",
    "\n",
    "    content_groups = re.findall(r'\\((.*?)\\)', content_string)\n",
    "    \n",
    "    # Valid characters are alphabetic only\n",
    "    valid_actions = [char for char in action_string if char.isalpha()]\n",
    "    \n",
    "    # Pair valid actions with corresponding content groups\n",
    "    concatenated_output = []\n",
    "    count = 0\n",
    "    for i, char in enumerate(action_string):\n",
    "        if(char.isalpha()):\n",
    "            if i < len(content_groups):  # Ensure no out-of-bound errors\n",
    "                concatenated_output.append(f\"({char} -> {content_groups[i-count]})\")\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            count += 1\n",
    "            if(char==\"|\"):\n",
    "                concatenated_output.append(\"|\")\n",
    "            else:\n",
    "                concatenated_output.append(f\"({char})\")\n",
    "\n",
    "    # Format as groups divided by \"|\"\n",
    "    return ''.join(concatenated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = map_action_to_content(self.data.loc[idx, 'action_bloc'], self.data.loc[idx, 'content_bloc'])\n",
    "        label = 1 if self.data.loc[idx, 'user_class'] == 'bot' else 0\n",
    "        return {\n",
    "            'text': text, \n",
    "            'label': label \n",
    "        }\n",
    "\n",
    "dataset = UserDataset(data)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter: subclass of Python's dictionary used for counting hashable objects, in this case, tokens (words).\n",
    "# OrderedDict: subclass of Python's dictionary that remembers the insertion order of keys. It is used to store tokens in a specific order based on frequency.\n",
    "from collections import Counter, OrderedDict\n",
    "# re: A module for working with regular expressions, used to manipulate and clean text.\n",
    "import re\n",
    "\n",
    "# Token counts and vocab creation\n",
    "# Initializes an empty Counter object to hold the frequency of each token in the dataset.\n",
    "token_counts = Counter()\n",
    "\n",
    "# Define tokenizer\n",
    "def tokenizer(text):\n",
    "    # Define the regex pattern to match tokens like (r -> E) and the pipe '|'\n",
    "    pattern = r'\\(.*?\\)|\\|'\n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the training data and populate token_counts\n",
    "for entry in train_dataset:  # Assuming train_dataset is a dataset with 'text'\n",
    "    line = entry['text']\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Sort tokens by frequency\n",
    "# token_counts.items() returns the tokens and their respective counts as a list of tuples (e.g., [(token1, count1), (token2, count2), ...])\n",
    "# key=lambda x: x[1] means that the sorting is based on the count (x[1]), which is the second element of each tuple\n",
    "# reverse=True means that the most frequent tokens appear first in the sorted list.\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create an ordered dictionary for the vocab\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# The padding token (pad) is used to ensure that all sequences in a batch have the same length.\n",
    "# The unknown token (unk) is used to represent words that are not found in the model's vocabulary (the top 69021 words in your case).\n",
    "# Any word that doesn't appear in the vocabulary is replaced by the unk token during tokenization.\n",
    "# This is critical for handling unseen words during inference, where the model encounters words that were not present in the training data.\n",
    "# Create vocab dictionary with special tokens\n",
    "# Initializes the vocab dictionary with two special tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "for idx, (token, count) in enumerate(ordered_dict.items(), start=2):  # Start from 2 to skip the special tokens\n",
    "    vocab[token] = idx\n",
    "\n",
    "\n",
    "# Print the vocabulary size (should be 69023)\n",
    "print('Vocab-size:', len(vocab))\n",
    "print('vocab', vocab)\n",
    "# --- Rationale:\n",
    "#\n",
    "# By assigning frequent words lower indices, we can optimize memory and computational efficiency.\n",
    "# Words that appear infrequently can either be assigned higher indices (in case we want to keep them) or omitted from the vocabulary entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action T|⚂T|⚅T□TT□r⚀r⚀r|⚀r|⚀r□r⚀r|⚂rTT□r□r⚀r□r|⚀r⚀r⚀r\n",
    "\n",
    "# content (t)|(t)|(Et)(E)(Et)(qt)(Et)(EHUt)|(Et)|(mUt)(HHHHHHt)(qt)|(qt)(E)(Et)(mmmqt)(Et)(HUt)(Ut)|(qt)(mqt)(EHUt)\n",
    "\n",
    "# text (T -> t)|(⚂)(T -> t)|(⚅)(T -> Et)(□)(T -> E)(T -> Et)(□)(r -> qt)(⚀)(r -> Et)(⚀)(r -> EHUt)|(⚀)(r -> Et)|(⚀)(r -> mUt)(□)\n",
    "\n",
    "def encode(tokens):\n",
    "    #If the token does not exist in the vocab, the function returns the index of the <unk>\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "# Example usage\n",
    "print(encode(tokenizer('(T -> t)|(⚂)(T -> t)|(⚅)(T -> Et)(□)(T -> E)(T -> Et)(□)(r -> qt)(⚀)(r -> Et)(⚀)(r -> EHUt)|(⚀)(r -> Et)|(⚀)(r -> mUt)(□)')))  # Should output something like [11, 7, 35, 457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: this code may be very slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the manual vocab creation process from earlier\n",
    "# Assuming `vocab` and `tokenizer` are already defined\n",
    "\n",
    "#text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "# Updated text pipeline\n",
    "text_pipeline = lambda x: [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(x)]\n",
    "\n",
    "label_pipeline = lambda x: float(x)  # Convert to float to match the output\n",
    "\n",
    "# Batch collation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for entry in batch:  # Each 'entry' is a dictionary with 'text' and 'label'\n",
    "        _label = entry['label']\n",
    "        _text = entry['text']\n",
    "\n",
    "        # Process labels and text\n",
    "        label_list.append(label_pipeline(_label))  # Convert labels using label_pipeline\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # Convert text to indices\n",
    "\n",
    "        # Store processed text and its length\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # Convert lists to tensors and pad sequences\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----  Example usage with DataLoader -----#\n",
    "## Take a small batch\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "\n",
    "# Print the output batch\n",
    "print(\"Text batch:\", text_batch)\n",
    "print(\"Label batch:\", label_batch)\n",
    "print(\"Length batch:\", length_batch)\n",
    "print(\"Text batch shape:\", text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batching the datasets\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "print(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store training and validation metrics for each epoch\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "valid_accuracies = []\n",
    "valid_losses = []\n",
    "\n",
    "num_epochs = 20\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Training loop with metrics storage\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(val_dl)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_accuracies.append(acc_train)\n",
    "    train_losses.append(loss_train)\n",
    "    valid_accuracies.append(acc_valid)\n",
    "    valid_losses.append(loss_valid)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} - accuracy: {acc_train:.4f}, val_accuracy: {acc_valid:.4f}')\n",
    "\n",
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('images/lstm-accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Function to preprocess and predict a single account\n",
    "def predict_account(text, model, vocab):\n",
    "    model.eval()\n",
    "    # Tokenize and encode the input text using the same tokenizer and vocab as used during training\n",
    "    tokens = tokenizer(text)\n",
    "    encoded_text = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    # Convert the tokens to tensor and add batch dimension\n",
    "    text_tensor = torch.tensor(encoded_text).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    lengths_tensor = torch.tensor([len(encoded_text)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(text_tensor, lengths_tensor)[:, 0]\n",
    "\n",
    "    # Apply threshold of 0.5 for binary classification\n",
    "    prediction_label = 1 if prediction >= 0.5 else 0\n",
    "    return prediction.item(), prediction_label\n",
    "\n",
    "# List to store prediction results\n",
    "results = []\n",
    "\n",
    "# Iterate over all samples in the test dataset\n",
    "for sample in test_dataset:\n",
    "    account_string = sample['text']\n",
    "    true_label = sample['label']\n",
    "    predicted_value, predicted_label = predict_account(account_string, model, vocab)\n",
    "\n",
    "    # Append the data for each comment\n",
    "    results.append({\n",
    "        \"comment\": account_string,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_value\": predicted_value,\n",
    "        \"predicted_label\": predicted_label\n",
    "    })\n",
    "\n",
    "# Convert results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "#given the path from job\n",
    "# results_df.to_csv(\"./rnn/only_action_method/results/predictions-both.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Collect predictions and true labels from the validation set\n",
    "for sample in test_dataset:\n",
    "    account_string = sample['text']\n",
    "    true_label = sample['label']\n",
    "    predicted_value, predicted_label = predict_account(account_string, model, vocab)  # Adjust if predict_comment takes a batch\n",
    "    all_preds.append(predicted_label)\n",
    "    all_labels.append(true_label)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('images/lstm-confusion.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "y_test = all_labels\n",
    "y_pred = all_preds\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multiclass\n",
    "recall = recall_score(y_test, y_pred, average='weighted')        # Use 'weighted' for multiclass\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')                # Use 'weighted' for multiclass\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
